---
title: "Homework 5"
output:
  html_document: default
  html_notebook: default
---

## Problem 1

### 1.1

Train a classification tree:
```{r}
library(rpart)
load("../HW03/zip.RData")
set.seed(10)
dat.tree <- rpart(V1 ~ ., data=dat$train, method="class", control=rpart.control(cp=0.0001))
print(dat.tree)
printcp(dat.tree)
```

Plot CV error vs. `cp` values:
```{r}
plotcp(dat.tree)
```

### 1.2

Using the one-standard-error rule, the best `cp` value is 0.00247117, and the corresponding CV error is $$0.54709\times 0.096376 = .05272635$$.

### 1.3

Prune the tree with the best `cp`:

```{r}
best.cp <- 0.00247117
dat.pruned <- prune(dat.tree, cp=best.cp)
print(dat.pruned)
plot(dat.pruned, main = "Pruned Classification Tree")
text(dat.pruned)
```

### 1.4

Apply the pruned tree to the test data:
```{r}
y.test.pruned <- predict(dat.pruned, dat$test, type="class")
mean(y.test.pruned != dat$test[,1])
```

## Problem 2

### 2.1

Generate a bootstrap sample set:
```{r}
set.seed(20)
ind <- sample(1:nrow(dat$train),nrow(dat$train),replace=TRUE)
boot <- dat$train[ind,]
```

Number of distinct observations in the resampled dataset:
```{r}
length(unique(ind))
```

### 2.2

Train a classifcation tree and prune it using the best `cp` value obtained in Problem 1:
```{r}
dat.boot <- rpart(V1 ~ ., data=boot, method="class", control=rpart.control(cp=0.0001))
dat.boot <- prune(dat.boot, cp=best.cp)
plot(dat.boot, main = "Pruned Classification Tree")
text(dat.boot)
```

Obviously, the tree is not the same as the tree obtained in Problem 1.

### 2.3

Train classification trees on 25 bootstrap sample sets:
```{r}
generate.trees <- function(t) {
  trees <- list()
  for (i in 1:t) {
    set.seed(i)
    ind <- sample(1:nrow(dat$train),nrow(dat$train),replace=TRUE)
    boot <- dat$train[ind,]
    dat.boot <- rpart(V1 ~ ., data=boot, method="class", control=rpart.control(cp=0.0001))
    dat.boot <- prune(dat.boot, cp=best.cp)
    trees[[i]] <- dat.boot
  }
  trees
}
```


```{r}
trees <- generate.trees(25)
test.errors <- sapply(trees, function(tr) {
  y.test.boot <- predict(tr, dat$test, type="class")
  mean(y.test.boot != dat$test[,1])
})
test.errors
```

Mean test error rate:
```{r}
mean(test.errors)
```

### 2.4

Use majority vote to get an aggregated classifier:

```{r}
bagging.test.err <- function(trees) {
  t <- length(trees)
  y.test.bag.mat <- matrix(0, nrow=nrow(dat$test), ncol=t)
  for (i in 1:t) y.test.bag.mat[,i] <- as.numeric(as.character(predict(trees[[i]], dat$test, type="class")))
  y.test.bag <- as.factor(sapply(1:nrow(dat$test), function(i) {
    tab <- table(y.test.bag.mat[i,])
    names(tab)[which.max(tab)]
  }))
  mean(y.test.bag != dat$test[,1])
}
bagging.test.err(trees)
```

### 2.5

Try baggin with different `t` values:

```{r}
trees.tot <- generate.trees(40)
t.vals <- 1:40
test.errors <- c()
for (t in t.vals) {
  trees <- trees.tot[1:t]
  test.errors <- c(test.errors, bagging.test.err(trees))
}
test.errors
```

Plot test error vs. `t` values:
```{r}
library(ggplot2)
ggplot(data=data.frame(t.vals, test.errors), aes(t.vals, test.errors)) +
  geom_line() + labs(x="t", y="test error")
```

We notice that larger `t` value does not always lead to better result.