---
title: "Homework 5"
output:
  html_notebook: default
  html_document: default
---

## Problem 1

### 1.1

Train a classification tree:
```{r}
library(rpart)
load("../HW03/zip.RData")
set.seed(10)
dat.tree <- rpart(V1 ~ ., data=dat$train, method="class", control=rpart.control(cp=0.0001))
print(dat.tree)
printcp(dat.tree)
```

Plot CV error vs. `cp` values:
```{r}
plotcp(dat.tree)
```

### 1.2

Using the one-standard-error rule, the best `cp` value is 0.00247117, and the corresponding CV error is $$0.54709\times 0.096376 = .05272635$$.

### 1.3

Prune the tree with the best `cp`:

```{r}
best.cp <- 0.00247117
dat.pruned <- prune(dat.tree, cp=best.cp)
print(dat.pruned)
```

Draw the tree:
```{r}
library(rpart.plot)
prp(dat.pruned, roundint=F)
```

### 1.4

Apply the pruned tree to the test data:
```{r}
y.test.pruned <- predict(dat.pruned, dat$test, type="class")
mean(y.test.pruned != dat$test[,1])
```

## Problem 2

### 2.1

Generate a bootstrap sample set:
```{r}
set.seed(20)
ind <- sample(1:nrow(dat$train),nrow(dat$train),replace=TRUE)
boot <- dat$train[ind,]
```

Number of distinct observations in the resampled dataset:
```{r}
length(unique(ind))
```

### 2.2

Train a classifcation tree and prune it using the best `cp` value obtained in Problem 1:
```{r}
dat.boot <- rpart(V1 ~ ., data=boot, method="class", control=rpart.control(cp=0.0001))
dat.boot <- prune(dat.boot, cp=best.cp)
prp(dat.boot, roundint=F)
```

Obviously, the tree is not the same as the tree obtained in Problem 1.

### 2.3

Train classification trees on 25 bootstrap sample sets:
```{r}
generate.trees <- function(t) {
  trees <- list()
  for (i in 1:t) {
    set.seed(i)
    ind <- sample(1:nrow(dat$train),nrow(dat$train),replace=TRUE)
    boot <- dat$train[ind,]
    dat.boot <- rpart(V1 ~ ., data=boot, method="class", control=rpart.control(cp=0.0001))
    dat.boot <- prune(dat.boot, cp=best.cp)
    trees[[i]] <- dat.boot
  }
  trees
}
```


```{r}
trees <- generate.trees(25)
test.errors <- sapply(trees, function(tr) {
  y.test.boot <- predict(tr, dat$test, type="class")
  mean(y.test.boot != dat$test[,1])
})
test.errors
```

Mean test error rate:
```{r}
mean(test.errors)
```

### 2.4

Use majority vote to get an aggregated classifier:

```{r}
bagging.test.err <- function(trees) {
  t <- length(trees)
  y.test.bag.mat <- matrix(0, nrow=nrow(dat$test), ncol=t)
  for (i in 1:t) y.test.bag.mat[,i] <- as.numeric(as.character(predict(trees[[i]], dat$test, type="class")))
  y.test.bag <- as.factor(sapply(1:nrow(dat$test), function(i) {
    tab <- table(y.test.bag.mat[i,])
    names(tab)[which.max(tab)]
  }))
  mean(y.test.bag != dat$test[,1])
}
bagging.test.err(trees)
```

### 2.5

Try bagging with different `t` values:

```{r}
trees.tot <- generate.trees(40)
t.vals <- 1:40
test.errors <- c()
for (t in t.vals) {
  trees <- trees.tot[1:t]
  test.errors <- c(test.errors, bagging.test.err(trees))
}
test.errors
```

Plot test error vs. `t` values:
```{r}
library(ggplot2)
ggplot(data=data.frame(t.vals, test.errors), aes(t.vals, test.errors)) +
  geom_line() + labs(x="t", y="test error")
```

We notice that larger `t` value does not always lead to better result.

## Problem 3

### 3.1
Train a random forest and plot OOB error vs. number of trees:
```{r}
library(randomForest)
set.seed(10)
dat.rf <- randomForest(V1 ~ ., data=dat$train, ntree=5000)
oob.errors <- dat.rf$err.rate[,1]
ggplot(data=data.frame(n=1:5000, oob.errors), aes(n, oob.errors)) +
  geom_line() + labs(x="trees", y="OOB error")
```

### 3.2

Calculate the lowest OOB error and corresponding number of trees:
```{r}
print(min(oob.errors))
which(oob.errors == min(oob.errors))
```

### 3.3

Re-train the random forest using the optimum number of trees (511) obtained in Part 2:
```{r}
set.seed(10)
dat.rf <- randomForest(V1 ~ ., data=dat$train, ntree=511)
y.test.rf <- predict(dat.rf, dat$test, type="response")
mean(y.test.rf != dat$test[,1])
```

## Problem 4

### 4.1

Use k-means clustering to cluster the training data with `k`=3, and report the number of samples in each cluster
```{r}
set.seed(10)
dat.kmeans <- kmeans(dat$train[,-1], 3, nstart=20)
dat.kmeans$size
```

### 4.2

Plot the average image of each cluster:
```{r}
conv.image <- function(vec)
{
mat <- matrix(as.numeric(vec), nrow=16, ncol=16)
mat <- -mat[, 16 : 1]
par(mar=c(0, 0, 0, 0))
image(mat, col=gray(seq(0, 1, 0.01)), xaxt='n', yaxt='n')
}

for (i in 1:3) conv.image(dat.kmeans$centers[i,])
```

### 4.3

Use the gap statistic to find the best `k`:
```{r warning=FALSE}
library(cluster)
set.seed(10)
system.time(dat.kmeans <- clusGap(dat$train[,-1], FUNcluster=kmeans, nstart=20, K.max=10, B=50))
```

```{r}
plot(dat.kmeans)
```

Using Tibshirani et al.'s "1-sd rule", the best `k` is 10:
```{r}
print(dat.kmeans, method="Tibs2001SEmax")
```
